{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f781006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from networks.segformer import *\n",
    "# For jupyter notebook below\n",
    "from EffSegformer import *\n",
    "from Transception_cnn import *\n",
    "\n",
    "from typing import Tuple\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "\n",
    "# From MISSFormer.py class BridgeLayer_4\n",
    "# From Transception.py line83 forward part\n",
    "# FromEfficientAttention to FuseEfficientAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58e012be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNReLU(nn.Module):\n",
    "    def __init__(self, c_in, c_out, kernel_size, stride=1, padding=1, activation=True):\n",
    "        super(ConvBNReLU, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            c_in, c_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=False\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(c_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        if self.activation:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "\n",
    "    def __init__(self, cin, cout):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            ConvBNReLU(cin, cout, 3, 1, padding=1),\n",
    "            ConvBNReLU(cout, cout, 3, stride=1, padding=1, activation=False)\n",
    "        )\n",
    "        self.conv1 = nn.Conv2d(cout, cout, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm2d(cout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        h = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn(x)\n",
    "        x = h + x\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class MiT_3_ResInception_cnn1(nn.Module):\n",
    "    def __init__(self, image_size, in_dim, key_dim, value_dim, layers, head_count=1, dil_conv=1, token_mlp='mix_skip', inception=\"1\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Hs=[56, 28, 14, 7]\n",
    "        self.Ws=[56, 28, 14, 7]\n",
    "        patch_sizes = [7, 3, 3, 3]\n",
    "        strides = [2, 2, 2, 2]\n",
    "        padding_sizes = [3, 1, 1, 1]\n",
    "        if dil_conv:  \n",
    "            dilation = 2  \n",
    "            patch_sizes1 = [7, 3, 3, 3]\n",
    "            dil_padding_sizes1 = [3, 2, 2, 2]    \n",
    "          \n",
    "        else:\n",
    "            dilation = 1\n",
    "            patch_sizes1 = [7, 3, 3, 3]\n",
    "            dil_padding_sizes1 = [3, 1, 1, 1]\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "        # 1 by 1 convolution to alter the dimension\n",
    "      \n",
    "        self.conv1_1_s1 = nn.Conv2d((len(inception)+1)*in_dim[0], in_dim[0], 1)\n",
    "        self.conv1_1_s2 = nn.Conv2d((len(inception)+1)*in_dim[1], in_dim[1], 1)\n",
    "        self.conv1_1_s3 = nn.Conv2d((len(inception)+1)*in_dim[2], in_dim[2], 1)\n",
    "        self.conv1_1_s4 = nn.Conv2d((len(inception)+1)*in_dim[3], in_dim[3], 1)\n",
    "\n",
    "        # patch_embed\n",
    "        # layers = [2, 2, 2, 2] dims = [64, 128, 320, 512]\n",
    "        self.patch_embed1 = OverlapPatchEmbeddings(image_size//2, patch_sizes[0], strides[0], padding_sizes[0], in_dim[0], in_dim[0])\n",
    "        \n",
    "        self.patch_embed2_1 = OverlapPatchEmbeddings_fuse(image_size//4, patch_sizes1[1], strides[1], dil_padding_sizes1[1],dilation, in_dim[0], in_dim[1])\n",
    "        # self.patch_embed2_2 = OverlapPatchEmbeddings_fuse(image_size//4, patch_sizes2[1], strides[1], dil_padding_sizes2[1],dilation, in_dim[0], in_dim[1])\n",
    "        \n",
    "        \n",
    "        self.patch_embed3_1 = OverlapPatchEmbeddings_fuse(image_size//8, patch_sizes1[2], strides[2], dil_padding_sizes1[2],dilation, in_dim[1], in_dim[2])\n",
    "        # self.patch_embed3_2 = OverlapPatchEmbeddings_fuse(image_size//8, patch_sizes2[2], strides[2], dil_padding_sizes2[2],dilation, in_dim[1], in_dim[2])\n",
    "\n",
    "        self.patch_embed4_1 = OverlapPatchEmbeddings_fuse(image_size//16, patch_sizes1[3], strides[3], dil_padding_sizes1[3],dilation, in_dim[2], in_dim[3])\n",
    "        # self.patch_embed4_2 = OverlapPatchEmbeddings_fuse(image_size//16, patch_sizes2[3], strides[3], dil_padding_sizes2[3],dilation, in_dim[2], in_dim[3])\n",
    "        \n",
    "        # inception branch\n",
    "        multiResBlock = {\n",
    "                        '15': MultiResBlock_15,\n",
    "                        '13': MultiResBlock_13,\n",
    "                        '1': MultiResBlock_1,\n",
    "                        '3': MultiResBlock_3,\n",
    "                        '5': MultiResBlock_5,\n",
    "                        }\n",
    "        \n",
    "       \n",
    "        self.resInception2_2 = multiResBlock[inception](in_dim[0],in_dim[1],branch=1,downsample=strides[1],alpha=1)\n",
    "        self.resInception3_2 = multiResBlock[inception](in_dim[1],in_dim[2],branch=1,downsample=strides[2],alpha=1)\n",
    "        self.resInception4_2 = multiResBlock[inception](in_dim[2],in_dim[3],branch=1,downsample=strides[3],alpha=1)\n",
    "        \n",
    "        # CNN Layer\n",
    "        self.block0 = DoubleConv(3, in_dim[0])\n",
    "        self.pool0 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # transformer encoder\n",
    "        self.block1 = nn.ModuleList([ \n",
    "            EfficientTransformerBlock(in_dim[0], key_dim[0], value_dim[0], head_count, token_mlp)\n",
    "        for _ in range(layers[0])])\n",
    "        self.norm1 = nn.LayerNorm(in_dim[0])\n",
    "\n",
    "        self.block2 = nn.ModuleList([\n",
    "            EfficientTransformerBlockFuse_res(in_dim[1], key_dim[1], value_dim[1], head_count, token_mlp)\n",
    "        for _ in range(layers[1])])\n",
    "        self.norm2 = nn.LayerNorm(in_dim[1])\n",
    "\n",
    "        self.block3 = nn.ModuleList([\n",
    "            EfficientTransformerBlockFuse_res(in_dim[2], key_dim[2], value_dim[2], head_count, token_mlp)\n",
    "        for _ in range(layers[2])])\n",
    "        self.norm3 = nn.LayerNorm(in_dim[2])\n",
    "\n",
    "        self.block4 = nn.ModuleList([\n",
    "            EfficientTransformerBlockFuse_res(in_dim[3], key_dim[3], value_dim[3], head_count, token_mlp)\n",
    "        for _ in range(layers[3])])\n",
    "        self.norm4 = nn.LayerNorm(in_dim[3])\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.shape[0]\n",
    "        outs = []\n",
    "\n",
    "        # stage 0: CNN\n",
    "        x = self.block0(x)# 224 224 64\n",
    "        x = self.pool0(x)#112 112 64\n",
    "        outs.append(x)\n",
    "    \n",
    "        # stage 1\n",
    "        x, H, W = self.patch_embed1(x)# 56 56 128\n",
    "        for blk in self.block1:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm1(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "      \n",
    "\n",
    "        # merge 2\n",
    "        # print(\"-------EN: Stage 2------\\n\\n\")\n",
    "        x1, H1, W1 = self.patch_embed2_1(x)#28 28 256\n",
    "        H2 = H1\n",
    "        W2 = W1\n",
    "        # print(\"\\n S2: H1:{}, H2:{}\".format(H1,H2))\n",
    "        _, nfx1_len, _ = x1.shape\n",
    "        x2 = self.resInception2_2(x)\n",
    "        _, nfx2_len, _ = x2.shape\n",
    "        # print(\"\\n x2 shape:\", x2.shape)\n",
    "        nfx_cat = torch.cat((x1,x2),1)\n",
    "\n",
    "        # stage 2\n",
    "\n",
    "        for blk in self.block2:\n",
    "            nfx_cat = blk(nfx_cat, nfx1_len, nfx2_len, H1, W1, H2, W2)\n",
    "        tx = self.norm2(nfx_cat)\n",
    "        # The mlp has been passed in blk, so next just split the sequence and \n",
    "        # reshape to spatial dimension\n",
    "        b,tx_len,_ = tx.shape\n",
    "        # z_total = []\n",
    "        map_mx_total = []\n",
    "        for nz in range(int(tx_len/nfx1_len)):\n",
    "            z = tx[:, nz*nfx1_len:(nz+1)*nfx1_len, :]\n",
    "            # z_total.append(z)\n",
    "            # print( z.shape)\n",
    "            map_mx = z.reshape(b,H1,W1,-1)\n",
    "            map_mx = map_mx.permute(0,3,1,2)\n",
    "            # print( \"\\nmap_mx: \",map_mx.shape)\n",
    "            map_mx_total.append(map_mx)\n",
    "\n",
    "        cat_maps = torch.cat(map_mx_total,1)\n",
    "        x = self.conv1_1_s2(cat_maps)\n",
    "        outs.append(x)\n",
    "\n",
    "        \n",
    "       # merge 3\n",
    "        x1, H1, W1 = self.patch_embed3_1(x) # 14 14 512\n",
    "        H2 = H1\n",
    "        W2 = W1\n",
    "        # print(\"\\n S3: H1:{}, H2:{}\".format(H1,H2))\n",
    "        _, nfx1_len, _ = x1.shape\n",
    "        x2 = self.resInception3_2(x)\n",
    "        _, nfx2_len, _ = x2.shape\n",
    "        nfx_cat = torch.cat((x1,x2),1)\n",
    "\n",
    "        # stage 3\n",
    "        for blk in self.block3:\n",
    "            nfx_cat = blk(nfx_cat, nfx1_len, nfx2_len, H1, W1, H2, W2)\n",
    "        tx = self.norm3(nfx_cat)\n",
    "\n",
    "        b,tx_len,_ = tx.shape\n",
    "        # z_total = []\n",
    "        map_mx_total = []\n",
    "        for nz in range(int(tx_len/nfx1_len)):\n",
    "            z = tx[:, nz*nfx1_len:(nz+1)*nfx1_len, :]\n",
    "            # z_total.append(z)\n",
    "            # print( z.shape)\n",
    "            map_mx = z.reshape(b,H1,W1,-1)\n",
    "            map_mx = map_mx.permute(0,3,1,2)\n",
    "            # print( \"\\nmap_mx: \",map_mx.shape)\n",
    "            map_mx_total.append(map_mx)\n",
    "\n",
    "        cat_maps = torch.cat(map_mx_total,1)\n",
    "        x = self.conv1_1_s3(cat_maps)\n",
    "        outs.append(x)\n",
    "\n",
    "\n",
    "        # merge 4\n",
    "      \n",
    "        x1, H1, W1 = self.patch_embed4_1(x)#7 7 1024\n",
    "        H2 = H1\n",
    "        W2 = W1\n",
    "        # print(\"\\n S4: H1:{}, H2:{}\".format(H1,H2))\n",
    "        _, nfx1_len, _ = x1.shape\n",
    "        x2 = self.resInception4_2(x)\n",
    "        _, nfx2_len, _ = x2.shape\n",
    "        nfx_cat = torch.cat((x1,x2),1)\n",
    "\n",
    "        # stage 4\n",
    "        for blk in self.block4:\n",
    "            nfx_cat = blk(nfx_cat, nfx1_len, nfx2_len, H1, W1, H2, W2)\n",
    "        tx = self.norm4(nfx_cat)\n",
    "        b,tx_len,_ = tx.shape\n",
    "        # z_total = []\n",
    "        map_mx_total = []\n",
    "        for nz in range(int(tx_len/nfx1_len)):\n",
    "            z = tx[:, nz*nfx1_len:(nz+1)*nfx1_len, :]\n",
    "            # z_total.append(z)\n",
    "            # print( z.shape)\n",
    "            map_mx = z.reshape(b,H1,W1,-1)\n",
    "            map_mx = map_mx.permute(0,3,1,2)\n",
    "            # print( \"\\nmap_mx: \",map_mx.shape)\n",
    "            map_mx_total.append(map_mx)\n",
    "\n",
    "        cat_maps = torch.cat(map_mx_total,1)\n",
    "        x = self.conv1_1_s4(cat_maps)\n",
    "        outs.append(x)\n",
    "\n",
    "\n",
    "        return outs# len=5\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19906693",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transception(nn.Module):\n",
    "    def __init__(self, num_classes=9, head_count=1, dil_conv=1, token_mlp_mode=\"mix_skip\", inception=\"135\"):\n",
    "        super().__init__()\n",
    "    \n",
    "        # Encoder\n",
    "        dims, key_dim, value_dim, layers = [[64, 128, 320, 512], [64, 128, 320, 512], [64, 128, 320, 512], [2, 2, 2, 2]]        \n",
    "        self.backbone = MiT_3_ResInception_cnn1(image_size=224, in_dim=dims, key_dim=key_dim, value_dim=value_dim, layers=layers,\n",
    "                            head_count=head_count, dil_conv=dil_conv, token_mlp=token_mlp_mode, inception=inception)\n",
    "        # self.backbone = MiT_3inception_padding(image_size=224, in_dim=dims, key_dim=key_dim, value_dim=value_dim, layers=layers,\n",
    "        #                     head_count=head_count, dil_conv=dil_conv, token_mlp=token_mlp_mode)\n",
    "\n",
    "        # Here options:(1) MiT_3inception->3 stages;(2) MiT->4 stages; \n",
    "        # (3)MiT_3inception_padding: padding before transformer after patch embedding (follow depthconcat)\n",
    "        # Decoder\n",
    "        d_base_feat_size = 7 #16 for 512 input size, and 7 for 224\n",
    "        in_out_chan = [[32, 64, 64, 64],[144, 128, 128, 128],[288, 320, 320, 320],[512, 512, 512, 512]]  # [dim, out_dim, key_dim, value_dim]\n",
    "\n",
    "        self.decoder_3 = MyDecoderLayer((d_base_feat_size, d_base_feat_size), in_out_chan[3], head_count, \n",
    "                                        token_mlp_mode, n_class=num_classes)\n",
    "        self.decoder_2 = MyDecoderLayer((d_base_feat_size*2, d_base_feat_size*2), in_out_chan[2], head_count,\n",
    "                                        token_mlp_mode, n_class=num_classes)\n",
    "        self.decoder_1 = MyDecoderLayer((d_base_feat_size*4, d_base_feat_size*4), in_out_chan[1], head_count, \n",
    "                                        token_mlp_mode, n_class=num_classes) \n",
    "        self.decoder_0 = MyDecoderLayer((d_base_feat_size*4, d_base_feat_size*4), in_out_chan[1], head_count, \n",
    "                                        token_mlp_mode, n_class=num_classes) \n",
    "        self.decoder_cnn = MyDecoderLayer((d_base_feat_size*8, d_base_feat_size*8), in_out_chan[0], head_count,\n",
    "                                        token_mlp_mode, n_class=num_classes, is_last=True)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #---------------Encoder-------------------------\n",
    "        if x.size()[1] == 1:\n",
    "            x = x.repeat(1,3,1,1)\n",
    "\n",
    "        output_enc = self.backbone(x)\n",
    "\n",
    "        b,c,_,_ = output_enc[3].shape\n",
    "\n",
    "        #---------------Decoder-------------------------     \n",
    "        tmp_3 = self.decoder_3(output_enc[4].permute(0,2,3,1).view(b,-1,c))\n",
    "        tmp_2 = self.decoder_2(tmp_3, output_enc[3].permute(0,2,3,1))\n",
    "        tmp_1 = self.decoder_1(tmp_2, output_enc[2].permute(0,2,3,1))\n",
    "        tmp_0 = self.decoder_0(tmp_1, output_enc[1].permute(0,2,3,1))\n",
    "        final_pred = self.decoder_cnn(tmp_0, output_enc[0].permute(0,2,3,1))\n",
    "\n",
    "        return tmp_0\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d65ea15c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, -1, 320]' is invalid for input of size 25088",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-85bf7e95de7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdil_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_mlp_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mix_skip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'13'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/students/yiwei/anaconda3/envs/missformer/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-2d67625c56ff>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#---------------Decoder-------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtmp_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mtmp_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtmp_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_enc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, -1, 320]' is invalid for input of size 25088"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = Transception(num_classes=9, head_count=1, dil_conv = 1, token_mlp_mode=\"mix_skip\", inception='13')\n",
    "    print(model(torch.rand(1, 3, 224, 224)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c943aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
