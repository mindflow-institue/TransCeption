{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3962bba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " --------The importing has been done!------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from segformer import *\n",
    "from typing import Tuple\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "print(\"\\n\\n --------The importing has been done!------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f60ae",
   "metadata": {},
   "source": [
    "The input size is \n",
    "data shape--------- torch.Size([1, 1, 224, 224]) torch.Size([1, 1, 224, 224])\n",
    "\n",
    "image: torch.Size([1, 1, 224, 224])\n",
    "\n",
    "label: torch.Size([1, 1, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7fff2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the input of size torch.Size([1, 1, 224, 224])\n",
      "Test the input of size torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.rand(1, 1, 224, 224)\n",
    "print(\"Test the input of size {}\".format(inputs.shape))\n",
    "if inputs.size()[1] == 1:\n",
    "    inputs = inputs.repeat(1,3,1,1)\n",
    "print(\"Test the input of size {}\".format(inputs.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52702296",
   "metadata": {},
   "source": [
    "# Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76fba998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        input  -> x:[B, D, H, W]\n",
    "        output ->   [B, D, H, W]\n",
    "    \n",
    "        in_channels:    int -> Embedding Dimension \n",
    "        key_channels:   int -> Key Embedding Dimension,   Best: (in_channels)\n",
    "        value_channels: int -> Value Embedding Dimension, Best: (in_channels or in_channels//2) \n",
    "        head_count:     int -> It divides the embedding dimension by the head_count and process each part individually\n",
    "        \n",
    "        Conv2D # of Params:  ((k_h * k_w * C_in) + 1) * C_out)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, key_channels, value_channels, head_count=1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.key_channels = key_channels\n",
    "        self.head_count = head_count\n",
    "        self.value_channels = value_channels\n",
    "\n",
    "        self.keys = nn.Conv2d(in_channels, key_channels, 1) \n",
    "        self.queries = nn.Conv2d(in_channels, key_channels, 1)\n",
    "        self.values = nn.Conv2d(in_channels, value_channels, 1)\n",
    "        self.reprojection = nn.Conv2d(value_channels, in_channels, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        n, _, h, w = input_.size()\n",
    "        \n",
    "        keys = self.keys(input_).reshape((n, self.key_channels, h * w))\n",
    "        queries = self.queries(input_).reshape(n, self.key_channels, h * w)\n",
    "        values = self.values(input_).reshape((n, self.value_channels, h * w))\n",
    "        \n",
    "        head_key_channels = self.key_channels // self.head_count\n",
    "        head_value_channels = self.value_channels // self.head_count\n",
    "        \n",
    "        attended_values = []\n",
    "        for i in range(self.head_count):\n",
    "            key = F.softmax(keys[\n",
    "                :,\n",
    "                i * head_key_channels: (i + 1) * head_key_channels,\n",
    "                :\n",
    "            ], dim=2)\n",
    "            \n",
    "            query = F.softmax(queries[\n",
    "                :,\n",
    "                i * head_key_channels: (i + 1) * head_key_channels,\n",
    "                :\n",
    "            ], dim=1)\n",
    "                        \n",
    "            value = values[\n",
    "                :,\n",
    "                i * head_value_channels: (i + 1) * head_value_channels,\n",
    "                :\n",
    "            ]            \n",
    "            \n",
    "            context = key @ value.transpose(1, 2) # dk*dv\n",
    "            attended_value = (context.transpose(1, 2) @ query).reshape(n, head_value_channels, h, w) # n*dv            \n",
    "            attended_values.append(attended_value)\n",
    "                \n",
    "        aggregated_values = torch.cat(attended_values, dim=1)\n",
    "        attention = self.reprojection(aggregated_values)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ebd0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Input  -> x (Size: (b, (H*W), d)), H, W\n",
    "        Output -> (b, (H*W), d)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, key_dim, value_dim, head_count=1, token_mlp='mix'):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(in_dim)\n",
    "        self.attn = EfficientAttention(in_channels=in_dim, key_channels=key_dim,\n",
    "                                       value_channels=value_dim, head_count=1)\n",
    "        self.norm2 = nn.LayerNorm(in_dim)\n",
    "        if token_mlp=='mix':\n",
    "            self.mlp = MixFFN(in_dim, int(in_dim*4))  \n",
    "        elif token_mlp=='mix_skip':\n",
    "            self.mlp = MixFFN_skip(in_dim, int(in_dim*4)) \n",
    "        else:\n",
    "            self.mlp = MLP_FFN(in_dim, int(in_dim*4))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, H, W) -> torch.Tensor:\n",
    "        norm_1 = self.norm1(x)\n",
    "        norm_1 = Rearrange('b (h w) d -> b d h w', h=H, w=W)(norm_1)\n",
    "        \n",
    "        attn = self.attn(norm_1)\n",
    "        attn = Rearrange('b d h w -> b (h w) d')(attn)\n",
    "        \n",
    "        tx = x + attn\n",
    "        mx = tx + self.mlp(self.norm2(tx), H, W)\n",
    "        return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68498648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = EfficientTransformerBlock(in_dim=64, key_dim=64, dim_value=64, head_count=1, token_mlp='mix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7d037c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test(torch.rand(1, 3136, 64), 56, 56).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dff64a7",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af41e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverlapPatchEmbeddings(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=7, stride=4, padding=1, in_ch=3, dim=768):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_ch, dim, patch_size, stride, padding)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        px = self.proj(x)\n",
    "        _, _, H, W = px.shape\n",
    "        fx = px.flatten(2).transpose(1, 2)\n",
    "        nfx = self.norm(fx)\n",
    "        return nfx, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124780f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverlapPatchEmbeddings_inception(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=7, stride=4, padding=1, in_ch=3, dim=768):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_ch, dim, patch_size, stride, padding)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        px = self.proj(x)\n",
    "        _, _, H, W = px.shape\n",
    "        fx = px.flatten(2).transpose(1, 2)\n",
    "        nfx = self.norm(fx)\n",
    "        return nfx, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "997bfa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiT(nn.Module):\n",
    "    def __init__(self, image_size, in_dim, key_dim, value_dim, layers, head_count=1, token_mlp='mix_skip'):\n",
    "        super().__init__()\n",
    "        patch_sizes = [7, 3, 3, 3]\n",
    "        strides = [4, 2, 2, 2]\n",
    "        padding_sizes = [3, 1, 1, 1]\n",
    "\n",
    "        \n",
    "        # patch_embed\n",
    "        # layers = [2, 2, 2, 2] dims = [64, 128, 320, 512]\n",
    "        self.patch_embed1 = OverlapPatchEmbeddings(image_size, patch_sizes[0], strides[0], padding_sizes[0], 3, in_dim[0])\n",
    "        self.patch_embed2 = OverlapPatchEmbeddings(image_size//4, patch_sizes[1], strides[1], padding_sizes[1],in_dim[0], in_dim[1])\n",
    "        self.patch_embed3 = OverlapPatchEmbeddings(image_size//8, patch_sizes[2], strides[2], padding_sizes[2],in_dim[1], in_dim[2])\n",
    "        self.patch_embed4 = OverlapPatchEmbeddings(image_size//16, patch_sizes[3], strides[3], padding_sizes[3],in_dim[2], in_dim[3])\n",
    "        \n",
    "        # transformer encoder\n",
    "        self.block1 = nn.ModuleList([ \n",
    "            EfficientTransformerBlock(in_dim[0], key_dim[0], value_dim[0], head_count, token_mlp)\n",
    "        for _ in range(layers[0])])\n",
    "        self.norm1 = nn.LayerNorm(in_dim[0])\n",
    "\n",
    "        self.block2 = nn.ModuleList([\n",
    "            EfficientTransformerBlock(in_dim[1], key_dim[1], value_dim[1], head_count, token_mlp)\n",
    "        for _ in range(layers[1])])\n",
    "        self.norm2 = nn.LayerNorm(in_dim[1])\n",
    "\n",
    "        self.block3 = nn.ModuleList([\n",
    "            EfficientTransformerBlock(in_dim[2], key_dim[2], value_dim[2], head_count, token_mlp)\n",
    "        for _ in range(layers[2])])\n",
    "        self.norm3 = nn.LayerNorm(in_dim[2])\n",
    "\n",
    "        self.block4 = nn.ModuleList([\n",
    "            EfficientTransformerBlock(in_dim[3], key_dim[3], value_dim[3], head_count, token_mlp)\n",
    "        for _ in range(layers[3])])\n",
    "        self.norm4 = nn.LayerNorm(in_dim[3])\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.shape[0]\n",
    "        outs = []\n",
    "\n",
    "        # stage 1 \n",
    "        x, H, W = self.patch_embed1(x)\n",
    "        print(x.shape)\n",
    "        for blk in self.block1:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm1(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 2\n",
    "        x, H, W = self.patch_embed2(x)\n",
    "        for blk in self.block2:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm2(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 3\n",
    "        x, H, W = self.patch_embed3(x)\n",
    "        for blk in self.block3:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm3(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 4\n",
    "        x, H, W = self.patch_embed4(x)\n",
    "        for blk in self.block4:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm4(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3684df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiT_inception(nn.Module):\n",
    "    def __init__(self, image_size, in_dim, key_dim, value_dim, layers, head_count=1, token_mlp='mix_skip'):\n",
    "        super().__init__()\n",
    "        patch_sizes = [7, 3, 3, 3]\n",
    "        strides = [4, 2, 2, 2]\n",
    "        padding_sizes = [3, 1, 1, 1]\n",
    "\n",
    "        \n",
    "        # patch_embed\n",
    "        # layers = [2, 2, 2, 2] dims = [64, 128, 320, 512]\n",
    "        self.patch_embed1 = OverlapPatchEmbeddings(image_size, patch_sizes[0], strides[0], padding_sizes[0], 3, in_dim[0])\n",
    "        self.patch_embed2 = OverlapPatchEmbeddings(image_size//4, patch_sizes[1], strides[1], padding_sizes[1],in_dim[0], in_dim[1])\n",
    "        self.patch_embed3 = OverlapPatchEmbeddings(image_size//8, patch_sizes[2], strides[2], padding_sizes[2],in_dim[1], in_dim[2])\n",
    "        self.patch_embed4 = OverlapPatchEmbeddings(image_size//16, patch_sizes[3], strides[3], padding_sizes[3],in_dim[2], in_dim[3])\n",
    "        \n",
    "        # transformer encoder\n",
    "        self.block1 = nn.ModuleList([ \n",
    "            EfficientTransformerBlock(in_dim[0], key_dim[0], value_dim[0], head_count, token_mlp)\n",
    "        for _ in range(layers[0])])\n",
    "        self.norm1 = nn.LayerNorm(in_dim[0])\n",
    "\n",
    "        self.block2 = nn.ModuleList([\n",
    "            EfficientTransformerBlock(in_dim[1], key_dim[1], value_dim[1], head_count, token_mlp)\n",
    "        for _ in range(layers[1])])\n",
    "        self.norm2 = nn.LayerNorm(in_dim[1])\n",
    "\n",
    "        self.block3 = nn.ModuleList([\n",
    "            EfficientTransformerBlock(in_dim[2], key_dim[2], value_dim[2], head_count, token_mlp)\n",
    "        for _ in range(layers[2])])\n",
    "        self.norm3 = nn.LayerNorm(in_dim[2])\n",
    "\n",
    "        self.block4 = nn.ModuleList([\n",
    "            EfficientTransformerBlock(in_dim[3], key_dim[3], value_dim[3], head_count, token_mlp)\n",
    "        for _ in range(layers[3])])\n",
    "        self.norm4 = nn.LayerNorm(in_dim[3])\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.shape[0]\n",
    "        outs = []\n",
    "\n",
    "        # stage 1 \n",
    "        x, H, W = self.patch_embed1(x)\n",
    "        print(x.shape)\n",
    "        for blk in self.block1:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm1(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 2\n",
    "        x, H, W = self.patch_embed2(x)\n",
    "        for blk in self.block2:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm2(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 3\n",
    "        x, H, W = self.patch_embed3(x)\n",
    "        for blk in self.block3:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm3(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 4\n",
    "        x, H, W = self.patch_embed4(x)\n",
    "        for blk in self.block4:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm4(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e36feaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3136, 64])\n",
      "The number of stages from encoder: 4\n",
      "The size of output from the 0 stage: torch.Size([1, 64, 56, 56])\n",
      "The size of output from the 1 stage: torch.Size([1, 128, 28, 28])\n",
      "The size of output from the 2 stage: torch.Size([1, 320, 14, 14])\n",
      "The size of output from the 3 stage: torch.Size([1, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "in_dim, layers = [[64, 128, 320, 512], [2, 2, 2, 2]]\n",
    "key_dim = dim_value = in_dim\n",
    "head_count = 1\n",
    "token_mlp_mode=\"mix_skip\"\n",
    "\n",
    "encoder = MiT(224, in_dim, key_dim, dim_value, layers, head_count, token_mlp_mode)\n",
    "output_enc = encoder(inputs)\n",
    "print(\"The number of stages from encoder: {}\".format(len(output_enc)))\n",
    "for i in range(len(output_enc)):\n",
    "    print(\"The size of output from the {} stage: {}\".format(i, output_enc[i].shape))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3196a78f",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b04f6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExpand(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, dim_scale=2, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.expand = nn.Linear(dim, 2*dim, bias=False) if dim_scale==2 else nn.Identity()\n",
    "        self.norm = norm_layer(dim // dim_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        # print(\"x_shape-----\",x.shape)\n",
    "        H, W = self.input_resolution\n",
    "        x = self.expand(x)\n",
    "        \n",
    "        B, L, C = x.shape\n",
    "        # print(x.shape)\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=2, p2=2, c=C//4)\n",
    "        x = x.view(B,-1,C//4)\n",
    "        x= self.norm(x.clone())\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7092ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalPatchExpand_X4(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, dim_scale=4, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.dim_scale = dim_scale\n",
    "        self.expand = nn.Linear(dim, 16*dim, bias=False)\n",
    "        self.output_dim = dim \n",
    "        self.norm = norm_layer(self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        x = self.expand(x)\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=self.dim_scale, p2=self.dim_scale, c=C//(self.dim_scale**2))\n",
    "        x = x.view(B,-1,self.output_dim)\n",
    "        x= self.norm(x.clone())\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "276c2cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecoderLayer(nn.Module):\n",
    "    def __init__(self, input_size, in_out_chan, head_count, token_mlp_mode, n_class=9,\n",
    "                 norm_layer=nn.LayerNorm, is_last=False):\n",
    "        super().__init__()\n",
    "        dims = in_out_chan[0]\n",
    "        out_dim = in_out_chan[1]\n",
    "        key_dim = in_out_chan[2]\n",
    "        value_dim = in_out_chan[3]\n",
    "        if not is_last:\n",
    "            self.concat_linear = nn.Linear(dims*2, out_dim)\n",
    "            # transformer decoder\n",
    "            self.layer_up = PatchExpand(input_resolution=input_size, dim=out_dim, dim_scale=2, norm_layer=norm_layer)\n",
    "            self.last_layer = None\n",
    "        else:\n",
    "            self.concat_linear = nn.Linear(dims*4, out_dim)\n",
    "            # transformer decoder\n",
    "            self.layer_up = FinalPatchExpand_X4(input_resolution=input_size, dim=out_dim, dim_scale=4, norm_layer=norm_layer)\n",
    "            # self.last_layer = nn.Linear(out_dim, n_class)\n",
    "            self.last_layer = nn.Conv2d(out_dim, n_class,1)\n",
    "            # self.last_layer = None\n",
    "\n",
    "        self.layer_former_1 = EfficientTransformerBlock(out_dim, key_dim, value_dim, head_count, token_mlp_mode)\n",
    "        self.layer_former_2 = EfficientTransformerBlock(out_dim, key_dim, value_dim, head_count, token_mlp_mode)\n",
    "       \n",
    "\n",
    "        def init_weights(self): \n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.LayerNorm):\n",
    "                    nn.init.ones_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.Conv2d):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "\n",
    "        init_weights(self)\n",
    "      \n",
    "    def forward(self, x1, x2=None):\n",
    "        if x2 is not None:# skip connection exist\n",
    "            print(\"x1 shape:\", x1.shape)\n",
    "            print(\"x2 shape:\", x2.shape)\n",
    "            b, h, w, c = x2.shape\n",
    "            x2 = x2.view(b, -1, c)\n",
    "            print(\"------\",x1.shape, x2.shape)\n",
    "            cat_x = torch.cat([x1, x2], dim=-1)\n",
    "            print(\"-----catx shape:\", cat_x.shape)\n",
    "            cat_linear_x = self.concat_linear(cat_x)\n",
    "            print('cat_linear_x shape:', cat_linear_x.shape)\n",
    "            tran_layer_1 = self.layer_former_1(cat_linear_x, h, w)\n",
    "            tran_layer_2 = self.layer_former_2(tran_layer_1, h, w)\n",
    "            \n",
    "            if self.last_layer:\n",
    "                out = self.last_layer(self.layer_up(tran_layer_2).view(b, 4*h, 4*w, -1).permute(0,3,1,2)) \n",
    "            else:\n",
    "                out = self.layer_up(tran_layer_2)\n",
    "        else:\n",
    "            # if len(x1.shape)>3:\n",
    "            #     x1 = x1.permute(0,2,3,1)\n",
    "            #     b, h, w, c = x1.shape\n",
    "            #     x1 = x1.view(b, -1, c)\n",
    "            print(\"x1 shape\",x1.shape)\n",
    "            out = self.layer_up(x1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb729e9",
   "metadata": {},
   "source": [
    "# The EfficientMISSFormer Architecture WO Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12c77d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MISSFormer_WO_Bridge(nn.Module):\n",
    "    def __init__(self, num_classes=9, head_count=1, token_mlp_mode=\"mix_skip\"):\n",
    "        super().__init__()\n",
    "    \n",
    "        # Encoder\n",
    "        dims, key_dim, value_dim, layers = [[64, 128, 320, 512], [64, 128, 320, 512], [64, 128, 320, 512], [2, 2, 2, 2]]        \n",
    "        self.backbone = MiT(image_size=224, in_dim=dims, key_dim=key_dim, value_dim=value_dim, layers=layers,\n",
    "                            head_count=head_count, token_mlp=token_mlp_mode)\n",
    "        \n",
    "        # Decoder\n",
    "        d_base_feat_size = 7 #16 for 512 input size, and 7 for 224\n",
    "        in_out_chan = [[32, 64, 64, 64],[144, 128, 128, 128],[288, 320, 320, 320],[512, 512, 512, 512]]  # [dim, out_dim, key_dim, value_dim]\n",
    "\n",
    "        self.decoder_3 = MyDecoderLayer((d_base_feat_size, d_base_feat_size), in_out_chan[3], head_count, \n",
    "                                        token_mlp_mode, n_class=num_classes)\n",
    "        self.decoder_2 = MyDecoderLayer((d_base_feat_size*2, d_base_feat_size*2), in_out_chan[2], head_count,\n",
    "                                        token_mlp_mode, n_class=num_classes)\n",
    "        self.decoder_1 = MyDecoderLayer((d_base_feat_size*4, d_base_feat_size*4), in_out_chan[1], head_count, \n",
    "                                        token_mlp_mode, n_class=num_classes) \n",
    "        self.decoder_0 = MyDecoderLayer((d_base_feat_size*8, d_base_feat_size*8), in_out_chan[0], head_count,\n",
    "                                        token_mlp_mode, n_class=num_classes, is_last=True)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #---------------Encoder-------------------------\n",
    "        if x.size()[1] == 1:\n",
    "            x = x.repeat(1,3,1,1)\n",
    "\n",
    "        output_enc = self.backbone(x)\n",
    "        \n",
    "        b,c,_,_ = output_enc[3].shape\n",
    "        \n",
    "        #---------------Decoder-------------------------   \n",
    "        #from the bottom to the top\n",
    "        tmp_3 = self.decoder_3(output_enc[3].permute(0,2,3,1).view(b,-1,c))\n",
    "        \n",
    "        print('\\n', \"stage2-----\")   \n",
    "        tmp_2 = self.decoder_2(tmp_3, output_enc[2].permute(0,2,3,1))\n",
    "        \n",
    "        print('\\n', \"stage1-----\")   \n",
    "        tmp_1 = self.decoder_1(tmp_2, output_enc[1].permute(0,2,3,1))\n",
    "        \n",
    "        print('\\n', \"stage0-----\")  \n",
    "        tmp_0 = self.decoder_0(tmp_1, output_enc[0].permute(0,2,3,1))\n",
    "        \n",
    "        print('\\n', '---------------Decoder------------------')\n",
    "        print(output_enc[3].shape)\n",
    "        print(tmp_3.shape)\n",
    "        print(tmp_2.shape)\n",
    "        print(tmp_1.shape)\n",
    "        print(tmp_0.shape)\n",
    "\n",
    "        return tmp_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "975f7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MISSFormer_WO_Bridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd7bc245",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3136, 64])\n",
      "x1 shape torch.Size([1, 49, 512])\n",
      "\n",
      " stage2-----\n",
      "x1 shape: torch.Size([1, 196, 256])\n",
      "x2 shape: torch.Size([1, 14, 14, 320])\n",
      "------ torch.Size([1, 196, 256]) torch.Size([1, 196, 320])\n",
      "-----catx shape: torch.Size([1, 196, 576])\n",
      "cat_linear_x shape: torch.Size([1, 196, 320])\n",
      "\n",
      " stage1-----\n",
      "x1 shape: torch.Size([1, 784, 160])\n",
      "x2 shape: torch.Size([1, 28, 28, 128])\n",
      "------ torch.Size([1, 784, 160]) torch.Size([1, 784, 128])\n",
      "-----catx shape: torch.Size([1, 784, 288])\n",
      "cat_linear_x shape: torch.Size([1, 784, 128])\n",
      "\n",
      " stage0-----\n",
      "x1 shape: torch.Size([1, 3136, 64])\n",
      "x2 shape: torch.Size([1, 56, 56, 64])\n",
      "------ torch.Size([1, 3136, 64]) torch.Size([1, 3136, 64])\n",
      "-----catx shape: torch.Size([1, 3136, 128])\n",
      "cat_linear_x shape: torch.Size([1, 3136, 64])\n",
      "\n",
      " ---------------Decoder------------------\n",
      "torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 196, 256])\n",
      "torch.Size([1, 784, 160])\n",
      "torch.Size([1, 3136, 64])\n",
      "torch.Size([1, 9, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 224, 224])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(torch.rand(1, 3, 224, 224)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca271de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fvcore.nn import flop_count_table\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "flops = FlopCountAnalysis(test, torch.rand(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2eac072",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3136, 64])\n",
      "x1 shape torch.Size([1, 49, 512])\n",
      "\n",
      " stage2-----\n",
      "x1 shape: torch.Size([1, 196, 256])\n",
      "x2 shape: torch.Size([1, 14, 14, 320])\n",
      "------ torch.Size([1, 196, 256]) torch.Size([1, 196, 320])\n",
      "-----catx shape: torch.Size([1, 196, 576])\n",
      "cat_linear_x shape: torch.Size([1, 196, 320])\n",
      "\n",
      " stage1-----\n",
      "x1 shape: torch.Size([1, 784, 160])\n",
      "x2 shape: torch.Size([1, 28, 28, 128])\n",
      "------ torch.Size([1, 784, 160]) torch.Size([1, 784, 128])\n",
      "-----catx shape: torch.Size([1, 784, 288])\n",
      "cat_linear_x shape: torch.Size([1, 784, 128])\n",
      "\n",
      " stage0-----\n",
      "x1 shape: torch.Size([1, 3136, 64])\n",
      "x2 shape: torch.Size([1, 56, 56, 64])\n",
      "------ torch.Size([1, 3136, 64]) torch.Size([1, 3136, 64])\n",
      "-----catx shape: torch.Size([1, 3136, 128])\n",
      "cat_linear_x shape: torch.Size([1, 3136, 64])\n",
      "\n",
      " ---------------Decoder------------------\n",
      "torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 196, 256])\n",
      "torch.Size([1, 784, 160])\n",
      "torch.Size([1, 3136, 64])\n",
      "torch.Size([1, 9, 224, 224])\n",
      "| module                            | #parameters or shape   | #flops     |\n",
      "|:----------------------------------|:-----------------------|:-----------|\n",
      "| model                             | 22.312M                | 4.527G     |\n",
      "|  backbone                         |  11.33M                |  2.397G    |\n",
      "|   backbone.patch_embed1           |   9.6K                 |   30.507M  |\n",
      "|    backbone.patch_embed1.proj     |    9.472K              |    29.503M |\n",
      "|    backbone.patch_embed1.norm     |    0.128K              |    1.004M  |\n",
      "|   backbone.patch_embed2           |   74.112K              |   58.305M  |\n",
      "|    backbone.patch_embed2.proj     |    73.856K             |    57.803M |\n",
      "|    backbone.patch_embed2.norm     |    0.256K              |    0.502M  |\n",
      "|   backbone.patch_embed3           |   0.37M                |   72.567M  |\n",
      "|    backbone.patch_embed3.proj     |    0.369M              |    72.253M |\n",
      "|    backbone.patch_embed3.norm     |    0.64K               |    0.314M  |\n",
      "|   backbone.patch_embed4           |   1.476M               |   72.379M  |\n",
      "|    backbone.patch_embed4.proj     |    1.475M              |    72.253M |\n",
      "|    backbone.patch_embed4.norm     |    1.024K              |    0.125M  |\n",
      "|   backbone.block1                 |   0.108M               |   0.489G   |\n",
      "|    backbone.block1.0              |    54.08K              |    0.244G  |\n",
      "|    backbone.block1.1              |    54.08K              |    0.244G  |\n",
      "|   backbone.norm1                  |   0.128K               |   1.004M   |\n",
      "|    backbone.norm1.weight          |    (64,)               |            |\n",
      "|    backbone.norm1.bias            |    (64,)               |            |\n",
      "|   backbone.block2                 |   0.413M               |   0.476G   |\n",
      "|    backbone.block2.0              |    0.206M              |    0.238G  |\n",
      "|    backbone.block2.1              |    0.206M              |    0.238G  |\n",
      "|   backbone.norm2                  |   0.256K               |   0.502M   |\n",
      "|    backbone.norm2.weight          |    (128,)              |            |\n",
      "|    backbone.norm2.bias            |    (128,)              |            |\n",
      "|   backbone.block3                 |   2.507M               |   0.731G   |\n",
      "|    backbone.block3.0              |    1.253M              |    0.365G  |\n",
      "|    backbone.block3.1              |    1.253M              |    0.365G  |\n",
      "|   backbone.norm3                  |   0.64K                |   0.314M   |\n",
      "|    backbone.norm3.weight          |    (320,)              |            |\n",
      "|    backbone.norm3.bias            |    (320,)              |            |\n",
      "|   backbone.block4                 |   6.37M                |   0.466G   |\n",
      "|    backbone.block4.0              |    3.185M              |    0.233G  |\n",
      "|    backbone.block4.1              |    3.185M              |    0.233G  |\n",
      "|   backbone.norm4                  |   1.024K               |   0.125M   |\n",
      "|    backbone.norm4.weight          |    (512,)              |            |\n",
      "|    backbone.norm4.bias            |    (512,)              |            |\n",
      "|  decoder_3                        |  7.42M                 |  25.941M   |\n",
      "|   decoder_3.concat_linear         |   0.525M               |            |\n",
      "|    decoder_3.concat_linear.weight |    (512, 1024)         |            |\n",
      "|    decoder_3.concat_linear.bias   |    (512,)              |            |\n",
      "|   decoder_3.layer_up              |   0.525M               |   25.941M  |\n",
      "|    decoder_3.layer_up.expand      |    0.524M              |    25.69M  |\n",
      "|    decoder_3.layer_up.norm        |    0.512K              |    0.251M  |\n",
      "|   decoder_3.layer_former_1        |   3.185M               |            |\n",
      "|    decoder_3.layer_former_1.norm1 |    1.024K              |            |\n",
      "|    decoder_3.layer_former_1.attn  |    1.051M              |            |\n",
      "|    decoder_3.layer_former_1.norm2 |    1.024K              |            |\n",
      "|    decoder_3.layer_former_1.mlp   |    2.132M              |            |\n",
      "|   decoder_3.layer_former_2        |   3.185M               |            |\n",
      "|    decoder_3.layer_former_2.norm1 |    1.024K              |            |\n",
      "|    decoder_3.layer_former_2.attn  |    1.051M              |            |\n",
      "|    decoder_3.layer_former_2.norm2 |    1.024K              |            |\n",
      "|    decoder_3.layer_former_2.mlp   |    2.132M              |            |\n",
      "|  decoder_2                        |  2.897M                |  0.808G    |\n",
      "|   decoder_2.concat_linear         |   0.185M               |   36.127M  |\n",
      "|    decoder_2.concat_linear.weight |    (320, 576)          |            |\n",
      "|    decoder_2.concat_linear.bias   |    (320,)              |            |\n",
      "|   decoder_2.layer_up              |   0.205M               |   40.768M  |\n",
      "|    decoder_2.layer_up.expand      |    0.205M              |    40.141M |\n",
      "|    decoder_2.layer_up.norm        |    0.32K               |    0.627M  |\n",
      "|   decoder_2.layer_former_1        |   1.253M               |   0.365G   |\n",
      "|    decoder_2.layer_former_1.norm1 |    0.64K               |    0.314M  |\n",
      "|    decoder_2.layer_former_1.attn  |    0.411M              |    0.12G   |\n",
      "|    decoder_2.layer_former_1.norm2 |    0.64K               |    0.314M  |\n",
      "|    decoder_2.layer_former_1.mlp   |    0.841M              |    0.244G  |\n",
      "|   decoder_2.layer_former_2        |   1.253M               |   0.365G   |\n",
      "|    decoder_2.layer_former_2.norm1 |    0.64K               |    0.314M  |\n",
      "|    decoder_2.layer_former_2.attn  |    0.411M              |    0.12G   |\n",
      "|    decoder_2.layer_former_2.norm2 |    0.64K               |    0.314M  |\n",
      "|    decoder_2.layer_former_2.mlp   |    0.841M              |    0.244G  |\n",
      "|  decoder_1                        |  0.483M                |  0.531G    |\n",
      "|   decoder_1.concat_linear         |   36.992K              |   28.901M  |\n",
      "|    decoder_1.concat_linear.weight |    (128, 288)          |            |\n",
      "|    decoder_1.concat_linear.bias   |    (128,)              |            |\n",
      "|   decoder_1.layer_up              |   32.896K              |   26.694M  |\n",
      "|    decoder_1.layer_up.expand      |    32.768K             |    25.69M  |\n",
      "|    decoder_1.layer_up.norm        |    0.128K              |    1.004M  |\n",
      "|   decoder_1.layer_former_1        |   0.206M               |   0.238G   |\n",
      "|    decoder_1.layer_former_1.norm1 |    0.256K              |    0.502M  |\n",
      "|    decoder_1.layer_former_1.attn  |    66.048K             |    77.07M  |\n",
      "|    decoder_1.layer_former_1.norm2 |    0.256K              |    0.502M  |\n",
      "|    decoder_1.layer_former_1.mlp   |    0.14M               |    0.16G   |\n",
      "|   decoder_1.layer_former_2        |   0.206M               |   0.238G   |\n",
      "|    decoder_1.layer_former_2.norm1 |    0.256K              |    0.502M  |\n",
      "|    decoder_1.layer_former_2.attn  |    66.048K             |    77.07M  |\n",
      "|    decoder_1.layer_former_2.norm2 |    0.256K              |    0.502M  |\n",
      "|    decoder_1.layer_former_2.mlp   |    0.14M               |    0.16G   |\n",
      "|  decoder_0                        |  0.183M                |  0.765G    |\n",
      "|   decoder_0.concat_linear         |   8.256K               |   25.69M   |\n",
      "|    decoder_0.concat_linear.weight |    (64, 128)           |            |\n",
      "|    decoder_0.concat_linear.bias   |    (64,)               |            |\n",
      "|   decoder_0.layer_up              |   65.664K              |   0.222G   |\n",
      "|    decoder_0.layer_up.expand      |    65.536K             |    0.206G  |\n",
      "|    decoder_0.layer_up.norm        |    0.128K              |    16.056M |\n",
      "|   decoder_0.last_layer            |   0.585K               |   28.901M  |\n",
      "|    decoder_0.last_layer.weight    |    (9, 64, 1, 1)       |            |\n",
      "|    decoder_0.last_layer.bias      |    (9,)                |            |\n",
      "|   decoder_0.layer_former_1        |   54.08K               |   0.244G   |\n",
      "|    decoder_0.layer_former_1.norm1 |    0.128K              |    1.004M  |\n",
      "|    decoder_0.layer_former_1.attn  |    16.64K              |    77.07M  |\n",
      "|    decoder_0.layer_former_1.norm2 |    0.128K              |    1.004M  |\n",
      "|    decoder_0.layer_former_1.mlp   |    37.184K             |    0.165G  |\n",
      "|   decoder_0.layer_former_2        |   54.08K               |   0.244G   |\n",
      "|    decoder_0.layer_former_2.norm1 |    0.128K              |    1.004M  |\n",
      "|    decoder_0.layer_former_2.attn  |    16.64K              |    77.07M  |\n",
      "|    decoder_0.layer_former_2.norm2 |    0.128K              |    1.004M  |\n",
      "|    decoder_0.layer_former_2.mlp   |    37.184K             |    0.165G  |\n"
     ]
    }
   ],
   "source": [
    "print(flop_count_table(flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feebf7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
