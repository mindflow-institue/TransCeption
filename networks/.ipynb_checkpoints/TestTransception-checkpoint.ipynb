{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab24c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from networks.segformer import *\n",
    "# For jupyter notebook below\n",
    "from Transception import *\n",
    "from EffSegformer import *\n",
    "from typing import Tuple\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e88c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiT(nn.Module):\n",
    "    def __init__(self, image_size, in_dim, key_dim, value_dim, layers, head_count=1, token_mlp='mix_skip'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Hs=[56, 28, 14, 7]\n",
    "        self.Ws=[56, 28, 14, 7]\n",
    "\n",
    "#         patch_sizes = [7, 3, 3, 3]\n",
    "        patch_sizes1 = [7, 3, 3, 3]\n",
    "        patch_sizes2 = [5, 1, 1, 1]\n",
    "\n",
    "        strides = [4, 2, 2, 2]\n",
    "        # padding_sizes = [3, 1, 1, 1]\n",
    "        dil_padding_sizes1 = [3, 0, 0, 0]\n",
    "        dil_padding_sizes2 = [3, 0, 0, 0]\n",
    "\n",
    "        # 1 by 1 convolution to alter the dimension\n",
    "        self.conv1_1_s1 = nn.Conv2d(2*in_dim[0], in_dim[0], 1)\n",
    "        self.conv1_1_s2 = nn.Conv2d(2*in_dim[1], in_dim[1], 1)\n",
    "        self.conv1_1_s3 = nn.Conv2d(2*in_dim[2], in_dim[2], 1)\n",
    "        self.conv1_1_s4 = nn.Conv2d(2*in_dim[3], in_dim[3], 1)\n",
    "\n",
    "        # patch_embed\n",
    "        # layers = [2, 2, 2, 2] dims = [64, 128, 320, 512]\n",
    "        self.patch_embed1_1 = OverlapPatchEmbeddings_fuse(image_size, patch_sizes1[0], strides[0], dil_padding_sizes1[0], 3, in_dim[0])\n",
    "        self.patch_embed1_2 = OverlapPatchEmbeddings_fuse(image_size, patch_sizes2[0], strides[0], dil_padding_sizes2[0], 3, in_dim[0])\n",
    "\n",
    "        self.patch_embed2_1 = OverlapPatchEmbeddings_fuse(image_size//4, patch_sizes1[1], strides[1], dil_padding_sizes1[1],in_dim[0], in_dim[1])\n",
    "        self.patch_embed2_2 = OverlapPatchEmbeddings_fuse(image_size//4, patch_sizes2[1], strides[1], dil_padding_sizes2[1],in_dim[0], in_dim[1])\n",
    "\n",
    "        self.patch_embed3_1 = OverlapPatchEmbeddings_fuse(image_size//8, patch_sizes1[2], strides[2], dil_padding_sizes1[2],in_dim[1], in_dim[2])\n",
    "        self.patch_embed3_2 = OverlapPatchEmbeddings_fuse(image_size//8, patch_sizes2[2], strides[2], dil_padding_sizes2[2],in_dim[1], in_dim[2])\n",
    "\n",
    "        self.patch_embed4_1 = OverlapPatchEmbeddings_fuse(image_size//16, patch_sizes1[3], strides[3], dil_padding_sizes1[3],in_dim[2], in_dim[3])\n",
    "        self.patch_embed4_2 = OverlapPatchEmbeddings_fuse(image_size//16, patch_sizes2[3], strides[3], dil_padding_sizes2[3],in_dim[2], in_dim[3])\n",
    "        \n",
    "        # transformer encoder\n",
    "        self.block1 = nn.ModuleList([ \n",
    "            EfficientTransformerBlockFuse(in_dim[0], key_dim[0], value_dim[0], head_count, token_mlp)\n",
    "        for _ in range(layers[0])])\n",
    "        self.norm1 = nn.LayerNorm(in_dim[0])\n",
    "\n",
    "        self.block2 = nn.ModuleList([\n",
    "            EfficientTransformerBlockFuse(in_dim[1], key_dim[1], value_dim[1], head_count, token_mlp)\n",
    "        for _ in range(layers[1])])\n",
    "        self.norm2 = nn.LayerNorm(in_dim[1])\n",
    "\n",
    "        self.block3 = nn.ModuleList([\n",
    "            EfficientTransformerBlockFuse(in_dim[2], key_dim[2], value_dim[2], head_count, token_mlp)\n",
    "        for _ in range(layers[2])])\n",
    "        self.norm3 = nn.LayerNorm(in_dim[2])\n",
    "\n",
    "        self.block4 = nn.ModuleList([\n",
    "            EfficientTransformerBlockFuse(in_dim[3], key_dim[3], value_dim[3], head_count, token_mlp)\n",
    "        for _ in range(layers[3])])\n",
    "        self.norm4 = nn.LayerNorm(in_dim[3])\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.shape[0]\n",
    "        outs = []\n",
    "\n",
    "        # stage 1\n",
    "        x1, H1, W1 = self.patch_embed1_1(x)\n",
    "        _, nfx1_len, _ = x1.shape\n",
    "        x2, H2, W2 = self.patch_embed1_2(x)\n",
    "        _, nfx2_len, _ = x2.shape\n",
    "        nfx_cat = torch.cat((x1,x2),1)\n",
    "\n",
    "\n",
    "        for blk in self.block1:\n",
    "            nfx_cat = blk(nfx_cat, nfx1_len, nfx2_len, H1, W1, H2, W2)\n",
    "        nfx_cat = self.norm1(nfx_cat)\n",
    "        mx1 = nfx_cat[:, :nfx1_len, :]\n",
    "        mx2 = nfx_cat[:, nfx1_len: :]\n",
    "        map_mx1 = mx1.view(1,H1,W1,-1)\n",
    "        map_mx2 = mx2.view(1,H2,W2,-1)\n",
    "        map_mx1 = map_mx1.permute(0,3,1,2)\n",
    "        map_mx2 = map_mx2.permute(0,3,1,2)\n",
    "#         print(\"check map_mx1 and map_mx2 before interpolation: \\n map_mx1.shape:{} \\n map_mx2.shape:{}\".format(map_mx1.shape, map_mx2.shape))\n",
    "        map_mx1 = F.interpolate(map_mx1,[self.Hs[0], self.Ws[0]])\n",
    "        cat_maps = torch.cat((map_mx1, map_mx2),1)\n",
    "        x = self.conv1_1_s1(cat_maps)\n",
    "        # x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 2\n",
    "#         print('------stage 2---\\n')\n",
    "        x1, H1, W1 = self.patch_embed2_1(x)\n",
    "        _, nfx1_len, _ = x1.shape\n",
    "        x2, H2, W2 = self.patch_embed2_2(x)\n",
    "        _, nfx2_len, _ = x2.shape\n",
    "        nfx_cat = torch.cat((x1,x2),1)\n",
    "#         print(\"H1 W1{} {}\\n H2 W2:{} {}\".format(H1, W1, H2, W2))\n",
    "\n",
    "        for blk in self.block2:\n",
    "            nfx_cat = blk(nfx_cat, nfx1_len, nfx2_len, H1, W1, H2, W2)\n",
    "        nfx_cat = self.norm2(nfx_cat)\n",
    "        mx1 = nfx_cat[:, :nfx1_len, :]\n",
    "        mx2 = nfx_cat[:, nfx1_len: :]\n",
    "        map_mx1 = mx1.view(1,H1,W1,-1)\n",
    "        map_mx2 = mx2.view(1,H2,W2,-1)\n",
    "        map_mx1 = map_mx1.permute(0,3,1,2)\n",
    "        map_mx2 = map_mx2.permute(0,3,1,2)\n",
    "#         print(\"check map_mx1 and map_mx2 before interpolation: \\n map_mx1.shape:{} \\n map_mx2.shape:{}\".format(map_mx1.shape, map_mx2.shape))\n",
    "        map_mx1 = F.interpolate(map_mx1,[self.Hs[1], self.Ws[1]])\n",
    "#         print(\"check map_mx1 and map_mx2 before interpolation: /n map_mx1.shape:{} /n map_mx2.shape:{}\".format(map_mx1.shape, map_mx2.shape))\n",
    "        cat_maps = torch.cat((map_mx1, map_mx2),1)\n",
    "        x = self.conv1_1_s2(cat_maps)\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 3\n",
    "        x1, H1, W1 = self.patch_embed3_1(x)\n",
    "        _, nfx1_len, _ = x1.shape\n",
    "        x2, H2, W2 = self.patch_embed3_2(x)\n",
    "        _, nfx2_len, _ = x2.shape\n",
    "        nfx_cat = torch.cat((x1,x2),1)\n",
    "\n",
    "        for blk in self.block3:\n",
    "            nfx_cat = blk(nfx_cat, nfx1_len, nfx2_len, H1, W1, H2, W2)\n",
    "        nfx_cat = self.norm3(nfx_cat)\n",
    "        mx1 = nfx_cat[:, :nfx1_len, :]\n",
    "        mx2 = nfx_cat[:, nfx1_len: :]\n",
    "        map_mx1 = mx1.view(1,H1,W1,-1)\n",
    "        map_mx2 = mx2.view(1,H2,W2,-1)\n",
    "        map_mx1 = map_mx1.permute(0,3,1,2)\n",
    "        map_mx2 = map_mx2.permute(0,3,1,2)\n",
    "        map_mx1 = F.interpolate(map_mx1,[self.Hs[2], self.Ws[2]])\n",
    "        cat_maps = torch.cat((map_mx1, map_mx2),1)\n",
    "        x = self.conv1_1_s3(cat_maps)\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 4\n",
    "        x1, H1, W1 = self.patch_embed4_1(x)\n",
    "        _, nfx1_len, _ = x1.shape\n",
    "        x2, H2, W2 = self.patch_embed4_2(x)\n",
    "        _, nfx2_len, _ = x2.shape\n",
    "        nfx_cat = torch.cat((x1,x2),1)\n",
    "\n",
    "        for blk in self.block4:\n",
    "            nfx_cat = blk(nfx_cat, nfx1_len, nfx2_len, H1, W1, H2, W2)\n",
    "        nfx_cat = self.norm4(nfx_cat)\n",
    "        mx1 = nfx_cat[:, :nfx1_len, :]\n",
    "        mx2 = nfx_cat[:, nfx1_len: :]\n",
    "        map_mx1 = mx1.view(1,H1,W1,-1)\n",
    "        map_mx2 = mx2.view(1,H2,W2,-1)\n",
    "        map_mx1 = map_mx1.permute(0,3,1,2)\n",
    "        map_mx2 = map_mx2.permute(0,3,1,2)\n",
    "        map_mx1 = F.interpolate(map_mx1,[self.Hs[3], self.Ws[3]])\n",
    "        cat_maps = torch.cat((map_mx1, map_mx2),1)\n",
    "        x = self.conv1_1_s4(cat_maps)\n",
    "        outs.append(x)\n",
    "\n",
    "        return outs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "17ad387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecoderLayer(nn.Module):\n",
    "    def __init__(self, input_size, in_out_chan, head_count, token_mlp_mode, n_class=9,\n",
    "                 norm_layer=nn.LayerNorm, is_last=False):\n",
    "        super().__init__()\n",
    "        dims = in_out_chan[0]\n",
    "        out_dim = in_out_chan[1]\n",
    "        key_dim = in_out_chan[2]\n",
    "        value_dim = in_out_chan[3]\n",
    "        if not is_last:\n",
    "            self.concat_linear = nn.Linear(dims*2, out_dim)\n",
    "            # transformer decoder\n",
    "            self.layer_up = PatchExpand(input_resolution=input_size, dim=out_dim, dim_scale=2, norm_layer=norm_layer)\n",
    "            self.last_layer = None\n",
    "        else:\n",
    "            self.concat_linear = nn.Linear(dims*4, out_dim)\n",
    "            # transformer decoder\n",
    "            self.layer_up = FinalPatchExpand_X4(input_resolution=input_size, dim=out_dim, dim_scale=4, norm_layer=norm_layer)\n",
    "            # self.last_layer = nn.Linear(out_dim, n_class)\n",
    "            self.last_layer = nn.Conv2d(out_dim, n_class,1)\n",
    "            # self.last_layer = None\n",
    "\n",
    "        self.layer_former_1 = EfficientTransformerBlock(out_dim, key_dim, value_dim, head_count, token_mlp_mode)\n",
    "        self.layer_former_2 = EfficientTransformerBlock(out_dim, key_dim, value_dim, head_count, token_mlp_mode)\n",
    "       \n",
    "\n",
    "        def init_weights(self): \n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.LayerNorm):\n",
    "                    nn.init.ones_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.Conv2d):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "\n",
    "        init_weights(self)\n",
    "      \n",
    "    def forward(self, x1, x2=None):\n",
    "        if x2 is not None:# skip connection exist\n",
    "            b, h, w, c = x2.shape\n",
    "            x2 = x2.view(b, -1, c)\n",
    "            cat_x = torch.cat([x1, x2], dim=-1)\n",
    "            cat_linear_x = self.concat_linear(cat_x)\n",
    "            tran_layer_1 = self.layer_former_1(cat_linear_x, h, w)\n",
    "            tran_layer_2 = self.layer_former_2(tran_layer_1, h, w)\n",
    "            \n",
    "            if self.last_layer:\n",
    "                out = self.last_layer(self.layer_up(tran_layer_2).view(b, 4*h, 4*w, -1).permute(0,3,1,2)) \n",
    "            else:\n",
    "                out = self.layer_up(tran_layer_2)\n",
    "        else:\n",
    "            # if len(x1.shape)>3:\n",
    "            #     x1 = x1.permute(0,2,3,1)\n",
    "            #     b, h, w, c = x1.shape\n",
    "            #     x1 = x1.view(b, -1, c)\n",
    "            out = self.layer_up(x1)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "93b2bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transception(nn.Module):\n",
    "    def __init__(self, num_classes=9, head_count=1, token_mlp_mode=\"mix_skip\"):\n",
    "        super().__init__()\n",
    "    \n",
    "        # Encoder\n",
    "        dims, key_dim, value_dim, layers = [[64, 128, 320, 512], [64, 128, 320, 512], [64, 128, 320, 512], [2, 2, 2, 2]]        \n",
    "        self.backbone = MiT(image_size=224, in_dim=dims, key_dim=key_dim, value_dim=value_dim, layers=layers,\n",
    "                            head_count=head_count, token_mlp=token_mlp_mode)\n",
    "        \n",
    "        # Decoder\n",
    "        d_base_feat_size = 7 #16 for 512 input size, and 7 for 224\n",
    "        in_out_chan = [[32, 64, 64, 64],[144, 128, 128, 128],[288, 320, 320, 320],[512, 512, 512, 512]]  # [dim, out_dim, key_dim, value_dim]\n",
    "\n",
    "        self.decoder_3 = MyDecoderLayer((d_base_feat_size, d_base_feat_size), in_out_chan[3], head_count, \n",
    "                                        token_mlp_mode, n_class=num_classes)\n",
    "        self.decoder_2 = MyDecoderLayer((d_base_feat_size*2, d_base_feat_size*2), in_out_chan[2], head_count,\n",
    "                                        token_mlp_mode, n_class=num_classes)\n",
    "        self.decoder_1 = MyDecoderLayer((d_base_feat_size*4, d_base_feat_size*4), in_out_chan[1], head_count, \n",
    "                                        token_mlp_mode, n_class=num_classes) \n",
    "        self.decoder_0 = MyDecoderLayer((d_base_feat_size*8, d_base_feat_size*8), in_out_chan[0], head_count,\n",
    "                                        token_mlp_mode, n_class=num_classes, is_last=True)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #---------------Encoder-------------------------\n",
    "        if x.size()[1] == 1:\n",
    "            x = x.repeat(1,3,1,1)\n",
    "\n",
    "        output_enc = self.backbone(x)\n",
    "\n",
    "        b,c,_,_ = output_enc[3].shape\n",
    "\n",
    "        #---------------Decoder-------------------------     \n",
    "        tmp_3 = self.decoder_3(output_enc[3].permute(0,2,3,1).view(b,-1,c))\n",
    "        tmp_2 = self.decoder_2(tmp_3, output_enc[2].permute(0,2,3,1))\n",
    "        tmp_1 = self.decoder_1(tmp_2, output_enc[1].permute(0,2,3,1))\n",
    "        tmp_0 = self.decoder_0(tmp_1, output_enc[0].permute(0,2,3,1))\n",
    "\n",
    "        return tmp_0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7a5f7bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transception(\n",
      "  (backbone): MiT(\n",
      "    (conv1_1_s1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv1_1_s2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv1_1_s3): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv1_1_s4): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (patch_embed1_1): OverlapPatchEmbeddings_fuse(\n",
      "      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3), dilation=(2, 2))\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (patch_embed1_2): OverlapPatchEmbeddings_fuse(\n",
      "      (proj): Conv2d(3, 64, kernel_size=(5, 5), stride=(4, 4), padding=(3, 3), dilation=(2, 2))\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (patch_embed2_1): OverlapPatchEmbeddings_fuse(\n",
      "      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), dilation=(2, 2))\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (patch_embed2_2): OverlapPatchEmbeddings_fuse(\n",
      "      (proj): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), dilation=(2, 2))\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (patch_embed3_1): OverlapPatchEmbeddings_fuse(\n",
      "      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), dilation=(2, 2))\n",
      "      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (patch_embed3_2): OverlapPatchEmbeddings_fuse(\n",
      "      (proj): Conv2d(128, 320, kernel_size=(1, 1), stride=(2, 2), dilation=(2, 2))\n",
      "      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (patch_embed4_1): OverlapPatchEmbeddings_fuse(\n",
      "      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(2, 2), dilation=(2, 2))\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (patch_embed4_2): OverlapPatchEmbeddings_fuse(\n",
      "      (proj): Conv2d(320, 512, kernel_size=(1, 1), stride=(2, 2), dilation=(2, 2))\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (block1): ModuleList(\n",
      "      (0): EfficientTransformerBlockFuse(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): FuseEfficientAttention(\n",
      "          (keys): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (queries): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (values): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (reprojection): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp1): MixFFN_skip(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp2): MixFFN_skip(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): EfficientTransformerBlockFuse(\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): FuseEfficientAttention(\n",
      "          (keys): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (queries): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (values): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (reprojection): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp1): MixFFN_skip(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp2): MixFFN_skip(\n",
      "          (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (block2): ModuleList(\n",
      "      (0): EfficientTransformerBlockFuse(\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): FuseEfficientAttention(\n",
      "          (keys): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (queries): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (values): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (reprojection): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp1): MixFFN_skip(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp2): MixFFN_skip(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): EfficientTransformerBlockFuse(\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): FuseEfficientAttention(\n",
      "          (keys): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (queries): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (values): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (reprojection): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp1): MixFFN_skip(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp2): MixFFN_skip(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (block3): ModuleList(\n",
      "      (0): EfficientTransformerBlockFuse(\n",
      "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): FuseEfficientAttention(\n",
      "          (keys): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (queries): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (values): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (reprojection): Linear(in_features=320, out_features=320, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp1): MixFFN_skip(\n",
      "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp2): MixFFN_skip(\n",
      "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): EfficientTransformerBlockFuse(\n",
      "        (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): FuseEfficientAttention(\n",
      "          (keys): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (queries): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (values): Linear(in_features=320, out_features=320, bias=True)\n",
      "          (reprojection): Linear(in_features=320, out_features=320, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp1): MixFFN_skip(\n",
      "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp2): MixFFN_skip(\n",
      "          (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "          (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "    (block4): ModuleList(\n",
      "      (0): EfficientTransformerBlockFuse(\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): FuseEfficientAttention(\n",
      "          (keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (queries): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (reprojection): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp1): MixFFN_skip(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp2): MixFFN_skip(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): EfficientTransformerBlockFuse(\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): FuseEfficientAttention(\n",
      "          (keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (queries): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (reprojection): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp1): MixFFN_skip(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (mlp2): MixFFN_skip(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dwconv): DWConv(\n",
      "            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
      "          )\n",
      "          (act): GELU()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder_3): MyDecoderLayer(\n",
      "    (concat_linear): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (layer_up): PatchExpand(\n",
      "      (expand): Linear(in_features=512, out_features=1024, bias=False)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (layer_former_1): EfficientTransformerBlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): EfficientAttention(\n",
      "        (keys): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (queries): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (values): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (reprojection): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MixFFN_skip(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dwconv): DWConv(\n",
      "          (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_former_2): EfficientTransformerBlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): EfficientAttention(\n",
      "        (keys): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (queries): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (values): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (reprojection): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MixFFN_skip(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dwconv): DWConv(\n",
      "          (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder_2): MyDecoderLayer(\n",
      "    (concat_linear): Linear(in_features=576, out_features=320, bias=True)\n",
      "    (layer_up): PatchExpand(\n",
      "      (expand): Linear(in_features=320, out_features=640, bias=False)\n",
      "      (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (layer_former_1): EfficientTransformerBlock(\n",
      "      (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): EfficientAttention(\n",
      "        (keys): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (queries): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (values): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (reprojection): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MixFFN_skip(\n",
      "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "        (dwconv): DWConv(\n",
      "          (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_former_2): EfficientTransformerBlock(\n",
      "      (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): EfficientAttention(\n",
      "        (keys): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (queries): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (values): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (reprojection): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MixFFN_skip(\n",
      "        (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
      "        (dwconv): DWConv(\n",
      "          (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
      "        (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder_1): MyDecoderLayer(\n",
      "    (concat_linear): Linear(in_features=288, out_features=128, bias=True)\n",
      "    (layer_up): PatchExpand(\n",
      "      (expand): Linear(in_features=128, out_features=256, bias=False)\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (layer_former_1): EfficientTransformerBlock(\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): EfficientAttention(\n",
      "        (keys): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (queries): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (values): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (reprojection): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MixFFN_skip(\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dwconv): DWConv(\n",
      "          (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_former_2): EfficientTransformerBlock(\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): EfficientAttention(\n",
      "        (keys): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (queries): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (values): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (reprojection): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MixFFN_skip(\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (dwconv): DWConv(\n",
      "          (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder_0): MyDecoderLayer(\n",
      "    (concat_linear): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (layer_up): FinalPatchExpand_X4(\n",
      "      (expand): Linear(in_features=64, out_features=1024, bias=False)\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (last_layer): Conv2d(64, 9, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (layer_former_1): EfficientTransformerBlock(\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): EfficientAttention(\n",
      "        (keys): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (queries): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (values): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (reprojection): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MixFFN_skip(\n",
      "        (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dwconv): DWConv(\n",
      "          (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer_former_2): EfficientTransformerBlock(\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): EfficientAttention(\n",
      "        (keys): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (queries): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (values): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (reprojection): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MixFFN_skip(\n",
      "        (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dwconv): DWConv(\n",
      "          (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
      "        )\n",
      "        (act): GELU()\n",
      "        (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Transception(num_classes=9, head_count=1, token_mlp_mode=\"mix_skip\")\n",
    "# print(model(torch.rand(1, 3, 224, 224)).shape)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd2139e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
