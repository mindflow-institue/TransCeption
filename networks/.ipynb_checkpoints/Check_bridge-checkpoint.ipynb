{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b924313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import sqrt\n",
    "from re import X\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "from typing import Tuple\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Module, Conv2d, Parameter, Softmax\n",
    "from torchvision import models\n",
    "from torch.nn import init\n",
    "from torchinfo import summary\n",
    "# from torchstat import stat\n",
    "\n",
    "from functools import partial\n",
    "# from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "# from timm.models.layers import DropPath, trunc_normal_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06cb80f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP_FFN(nn.Module):\n",
    "    def __init__(self, c1, c2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(c1, c2)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(c2, c1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4832446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----New Bridge---------\n",
    "# Spatial Fuse module\n",
    "class MultiScaleAtten(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(MultiScaleAtten, self).__init__()\n",
    "        self.qkv_linear = nn.Linear(dim, dim * 3)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.num_head = 8\n",
    "        self.scale = (dim // self.num_head)**0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, num_blocks, _, _, C = x.shape  # (B, num_blocks, num_blocks, N, C)\n",
    "        qkv = self.qkv_linear(x).reshape(B, num_blocks, num_blocks, -1, 3, self.num_head, C // self.num_head).permute(4, 0, 1, 2, 5, 3, 6).contiguous() # (3, B, num_block, num_block, head, N, C)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        atten = q @ k.transpose(-1, -2).contiguous()\n",
    "        atten = self.softmax(atten)\n",
    "        atten_value = (atten @ v).transpose(-2, -3).contiguous().reshape(B, num_blocks, num_blocks, -1, C)\n",
    "        atten_value = self.proj(atten_value)  # (B, num_block, num_block, N, C)\n",
    "        return atten_value\n",
    "\n",
    "\n",
    "class InterTransBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(InterTransBlock, self).__init__()\n",
    "        self.SlayerNorm_1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.SlayerNorm_2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.Attention = MultiScaleAtten(dim)\n",
    "        self.mlp = MLP_FFN(dim,4*dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x  # (B, N, H)\n",
    "        x = self.SlayerNorm_1(x)\n",
    "\n",
    "        x = self.Attention(x)  # padding åˆ°right_size\n",
    "        x = h + x\n",
    "\n",
    "        h = x\n",
    "        x = self.SlayerNorm_2(x)\n",
    "\n",
    "        x = self.mlp(x)\n",
    "        x = h + x\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpatialAwareTrans(nn.Module):\n",
    "    def __init__(self, dim=64, num_sp_layer=1):  # (224*64, 112*128, 56*256, 28*256, 14*512) dim = 256\n",
    "        super(SpatialAwareTrans, self).__init__()\n",
    "        self.win_size_list = [8,4,2,1]\n",
    "        self.channels = [64, 64*2, 64*5, 64*8]\n",
    "        self.dim = dim\n",
    "        self.depth = 4\n",
    "        self.fc1 = nn.Linear(self.channels[0],dim)\n",
    "        self.fc2 = nn.Linear(self.channels[1],dim)\n",
    "        self.fc3 = nn.Linear(self.channels[2],dim)\n",
    "        self.fc4 = nn.Linear(self.channels[3],dim)\n",
    "\n",
    "        self.fc1_back = nn.Linear(dim, self.channels[0])\n",
    "        self.fc2_back = nn.Linear(dim, self.channels[1])\n",
    "        self.fc3_back = nn.Linear(dim, self.channels[2])\n",
    "        self.fc4_back = nn.Linear(dim, self.channels[3])\n",
    "\n",
    "        self.fc_back = nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.fc_back.append(nn.Linear(self.dim, self.channels[i]))\n",
    "      \n",
    "        self.num = num_sp_layer # the number of layers\n",
    "    \n",
    "\n",
    "        self.group_attention = []\n",
    "        for i in range(self.num):\n",
    "            self.group_attention.append(InterTransBlock(dim))\n",
    "        self.group_attention = nn.Sequential(*self.group_attention)\n",
    "        self.split_list = [8 * 8, 4 * 4, 2 * 2, 1 * 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # project channel dimension to 256\n",
    "        # print(\"Start spatial aware:------------\")\n",
    "        # print(f\"x_0:{x[0].shape}\")\n",
    "        # print(f\"x_1:{x[1].shape}\")\n",
    "        # print(f\"x_2:{x[2].shape}\")\n",
    "        # print(f\"x_3:{x[3].shape}\")\n",
    "\n",
    "        # utilize linear to project from other channel number to 256(C)\n",
    "        x[0] = self.fc1(x[0].permute(0,2,3,1))\n",
    "        x[1] = self.fc2(x[1].permute(0,2,3,1))\n",
    "        x[2] = self.fc3(x[2].permute(0,2,3,1))\n",
    "        x[3] = self.fc4(x[3].permute(0,2,3,1))\n",
    "        # x = [self.fc_module[i](item.permute(0, 2, 3, 1)) for i, item in enumerate(x)]  # [(B, H, W, C)]\n",
    "        # Patch Matching\n",
    "        # print(\"-----------------\")\n",
    "        for j, item in enumerate(x):\n",
    "            # print(f\"#{j} shape: {item.shape}\")\n",
    "            B, H, W, C = item.shape\n",
    "            win_size = self.win_size_list[j]\n",
    "            # print(f'window size: {win_size}')\n",
    "            item = item.reshape(B, H // win_size, win_size, W // win_size, win_size, C).permute(0, 1, 3, 2, 4, 5).contiguous()#([B,H/win,W/win, win,win,C])\n",
    "            # print(f'reshape first step:{item.shape}')\n",
    "            item = item.reshape(B, H // win_size, W // win_size, win_size * win_size, C).contiguous()#([B,H/win,W/win, win*win,C])\n",
    "            # print(f'reshape second step:{item.shape}')\n",
    "            x[j] = item\n",
    "        x = tuple(x)\n",
    "        x = torch.cat(x, dim=-2)  # (B, H // win, W // win, N, C)\n",
    "        # print(f\"\\n fuse the four level together:{x.shape}\")\n",
    "        \n",
    "        # Scale fusion\n",
    "        for i in range(self.num):\n",
    "            x = self.group_attention[i](x)  # (B, H // win_size, W // win_size, win_size*win_size, C)\n",
    "\n",
    "        x = torch.split(x, self.split_list, dim=-2)\n",
    "        x = list(x)\n",
    "        # patch reversion\n",
    "        # print(\"-------reversion----------\")\n",
    "        for j, item in enumerate(x):\n",
    "            B, num_blocks, _, N, C = item.shape\n",
    "            win_size = self.win_size_list[j]\n",
    "            item = item.reshape(B, num_blocks, num_blocks, win_size, win_size, C).permute(0, 1, 3, 2, 4, 5).contiguous().reshape(B, num_blocks*win_size, num_blocks*win_size, C)\n",
    "            item = self.fc_back[j](item).permute(0, 3, 1, 2).contiguous()\n",
    "            # print(f\"#{j} shape: {item.shape}\")\n",
    "            x[j] = item\n",
    "       \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8875bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixFFN_skip(nn.Module):\n",
    "    def __init__(self, c1, c2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(c1, c2)\n",
    "        self.dwconv = DWConv(c2)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(c2, c1)\n",
    "        self.norm1 = nn.LayerNorm(c2)\n",
    "        self.norm2 = nn.LayerNorm(c2)\n",
    "        self.norm3 = nn.LayerNorm(c2)\n",
    "    def forward(self, x: torch.Tensor, H, W) -> torch.Tensor:\n",
    "        ax = self.act(self.norm1(self.dwconv(self.fc1(x), H, W)+self.fc1(x)))\n",
    "        out = self.fc2(ax)\n",
    "        return out\n",
    "    \n",
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, groups=dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, H, W) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        # print('input in DWConv: {}'.format(x.shape))\n",
    "        tx = x.transpose(1, 2).view(B, C, H, W)\n",
    "        conv_x = self.dwconv(tx)\n",
    "        return conv_x.flatten(2).transpose(1, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f6f391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scale_reduce(nn.Module):\n",
    "    def __init__(self, dim, reduction_ratio):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        if(len(self.reduction_ratio)==4):\n",
    "            self.sr0 = nn.Conv2d(dim, dim, reduction_ratio[3], reduction_ratio[3])\n",
    "            self.sr1 = nn.Conv2d(dim*2, dim*2, reduction_ratio[2], reduction_ratio[2])\n",
    "            self.sr2 = nn.Conv2d(dim*5, dim*5, reduction_ratio[1], reduction_ratio[1])\n",
    "        \n",
    "        elif(len(self.reduction_ratio)==3):\n",
    "            self.sr0 = nn.Conv2d(dim*2, dim*2, reduction_ratio[2], reduction_ratio[2])\n",
    "            self.sr1 = nn.Conv2d(dim*5, dim*5, reduction_ratio[1], reduction_ratio[1])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        if(len(self.reduction_ratio)==4):\n",
    "            tem0 = x[:,:3136,:].reshape(B, 56, 56, C).permute(0, 3, 1, 2) \n",
    "            tem1 = x[:,3136:4704,:].reshape(B, 28, 28, C*2).permute(0, 3, 1, 2)\n",
    "            tem2 = x[:,4704:5684,:].reshape(B, 14, 14, C*5).permute(0, 3, 1, 2)\n",
    "            tem3 = x[:,5684:6076,:]\n",
    "\n",
    "            sr_0 = self.sr0(tem0).reshape(B, C, -1).permute(0, 2, 1)\n",
    "            sr_1 = self.sr1(tem1).reshape(B, C, -1).permute(0, 2, 1)\n",
    "            sr_2 = self.sr2(tem2).reshape(B, C, -1).permute(0, 2, 1)\n",
    "\n",
    "            reduce_out = self.norm(torch.cat([sr_0, sr_1, sr_2, tem3], -2))\n",
    "        \n",
    "        if(len(self.reduction_ratio)==3):\n",
    "            tem0 = x[:,:1568,:].reshape(B, 28, 28, C*2).permute(0, 3, 1, 2) \n",
    "            tem1 = x[:,1568:2548,:].reshape(B, 14, 14, C*5).permute(0, 3, 1, 2)\n",
    "            tem2 = x[:,2548:2940,:]\n",
    "\n",
    "            sr_0 = self.sr0(tem0).reshape(B, C, -1).permute(0, 2, 1)\n",
    "            sr_1 = self.sr1(tem1).reshape(B, C, -1).permute(0, 2, 1)\n",
    "            \n",
    "            reduce_out = self.norm(torch.cat([sr_0, sr_1, tem2], -2))\n",
    "        \n",
    "        return reduce_out\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "class M_EfficientSelfAtten(nn.Module):\n",
    "    def __init__(self, dim, head, reduction_ratio):\n",
    "        super().__init__()\n",
    "        self.head = head\n",
    "        self.reduction_ratio = reduction_ratio # list[1  2  4  8]\n",
    "        self.scale = (dim // head) ** -0.5\n",
    "        self.q = nn.Linear(dim, dim, bias=True)\n",
    "        self.kv = nn.Linear(dim, dim*2, bias=True)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        \n",
    "        if reduction_ratio is not None:\n",
    "            self.scale_reduce = Scale_reduce(dim,reduction_ratio)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, C = x.shape\n",
    "        q = self.q(x).reshape(B, N, self.head, C // self.head).permute(0, 2, 1, 3)\n",
    "\n",
    "        if self.reduction_ratio is not None:\n",
    "            x = self.scale_reduce(x)\n",
    "            \n",
    "        kv = self.kv(x).reshape(B, -1, 2, self.head, C // self.head).permute(2, 0, 3, 1, 4)\n",
    "        k, v = kv[0], kv[1]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn_score = attn.softmax(dim=-1)\n",
    "\n",
    "        x_atten = (attn_score @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        out = self.proj(x_atten)\n",
    "\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fb1b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BridgeLayer_new(nn.Module):\n",
    "    def __init__(self, dims, head, reduction_ratios):\n",
    "        super().__init__()\n",
    "        C = 64\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(dims)\n",
    "        self.attn = M_EfficientSelfAtten(dims, head, reduction_ratios)\n",
    "        self.norm2 = nn.LayerNorm(dims)\n",
    "        self.mixffn1 = MixFFN_skip(dims,dims*4)\n",
    "        self.mixffn2 = MixFFN_skip(dims*2,dims*8)\n",
    "        self.mixffn3 = MixFFN_skip(dims*5,dims*20)\n",
    "        self.mixffn4 = MixFFN_skip(dims*8,dims*32)\n",
    "        \n",
    "        self.scale_fuse_att = SpatialAwareTrans(dim=dims, num_sp_layer=1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        B = inputs[0].shape[0]\n",
    "        C = 64\n",
    "        H = 56\n",
    "        W = 56\n",
    "        if (type(inputs) == list):\n",
    "            inputs = self.scale_fuse_att(inputs)\n",
    "            c1, c2, c3, c4 = inputs\n",
    "            B, C, _, _= c1.shape\n",
    "            c1f = c1.permute(0, 2, 3, 1).reshape(B, -1, C)  # 3136*64\n",
    "            c2f = c2.permute(0, 2, 3, 1).reshape(B, -1, C)  # 1568*64\n",
    "            c3f = c3.permute(0, 2, 3, 1).reshape(B, -1, C)  # 980*64\n",
    "            c4f = c4.permute(0, 2, 3, 1).reshape(B, -1, C)  # 392*64\n",
    "            \n",
    "            # print(c1f.shape, c2f.shape, c3f.shape, c4f.shape)\n",
    "            inputs = torch.cat([c1f, c2f, c3f, c4f], -2)\n",
    "        else:\n",
    "            B,_,C = inputs.shape \n",
    "\n",
    "        tx1 = inputs + self.attn(self.norm1(inputs))\n",
    "        tx = self.norm2(tx1)\n",
    "\n",
    "\n",
    "        tem1 = tx[:,:3136,:].reshape(B, -1, C) \n",
    "        tem2 = tx[:,3136:4704,:].reshape(B, -1, C*2)\n",
    "        tem3 = tx[:,4704:5684,:].reshape(B, -1, C*5)\n",
    "        tem4 = tx[:,5684:6076,:].reshape(B, -1, C*8)\n",
    "\n",
    "        m1f = self.mixffn1(tem1, 56, 56).reshape(B, -1, C)\n",
    "        m2f = self.mixffn2(tem2, 28, 28).reshape(B, -1, C)\n",
    "        m3f = self.mixffn3(tem3, 14, 14).reshape(B, -1, C)\n",
    "        m4f = self.mixffn4(tem4, 7, 7).reshape(B, -1, C)\n",
    "\n",
    "        t1 = torch.cat([m1f, m2f, m3f, m4f], -2)\n",
    "        \n",
    "        tx2 = tx1 + t1\n",
    "\n",
    "\n",
    "        return tx2\n",
    "\n",
    "\n",
    "\n",
    "class BridgeBlock_new(nn.Module):\n",
    "    def __init__(self, dims, head, reduction_ratios):\n",
    "        super().__init__()\n",
    "        self.bridge_layer1 = BridgeLayer_new(dims, head, reduction_ratios)\n",
    "        self.bridge_layer2 = BridgeLayer_new(dims, head, reduction_ratios)\n",
    "        self.bridge_layer3 = BridgeLayer_new(dims, head, reduction_ratios)\n",
    "        self.bridge_layer4 = BridgeLayer_new(dims, head, reduction_ratios)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # print('Checking bridge')\n",
    "        bridge1 = self.bridge_layer1(x)\n",
    "        bridge2 = self.bridge_layer2(bridge1)\n",
    "        bridge3 = self.bridge_layer3(bridge2)\n",
    "        bridge4 = self.bridge_layer4(bridge3)\n",
    "\n",
    "        B,_,C = bridge4.shape\n",
    "        outs = []\n",
    "\n",
    "        sk1 = bridge4[:,:3136,:].reshape(B, 56, 56, C).permute(0,3,1,2) \n",
    "        sk2 = bridge4[:,3136:4704,:].reshape(B, 28, 28, C*2).permute(0,3,1,2) \n",
    "        sk3 = bridge4[:,4704:5684,:].reshape(B, 14, 14, C*5).permute(0,3,1,2) \n",
    "        sk4 = bridge4[:,5684:6076,:].reshape(B, 7, 7, C*8).permute(0,3,1,2) \n",
    "\n",
    "        outs.append(sk1)\n",
    "        outs.append(sk2)\n",
    "        outs.append(sk3)\n",
    "        outs.append(sk4)\n",
    "\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa6bc1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_ratios = [1, 2, 4, 8]\n",
    "bridge = BridgeBlock_new(64, 1, reduction_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9645f839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_enc[0]:torch.Size([1, 64, 56, 56])\n",
      "output_enc[1]:torch.Size([1, 128, 28, 28])\n",
      "output_enc[2]:torch.Size([1, 320, 14, 14])\n",
      "output_enc[3]:torch.Size([1, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "# output_enc[0]:torch.Size([1, 64, 56, 56])\n",
    "# output_enc[1]:torch.Size([1, 128, 28, 28])\n",
    "# output_enc[2]:torch.Size([1, 320, 14, 14])\n",
    "# output_enc[3]:torch.Size([1, 512, 7, 7])\n",
    "\n",
    "out_enc1 = torch.rand(1, 64, 56, 56)\n",
    "out_enc2 = torch.rand(1, 128, 28, 28)\n",
    "out_enc3 = torch.rand(1, 320, 14, 14)\n",
    "out_enc4 = torch.rand(1, 512, 7, 7)\n",
    "\n",
    "output_enc = []\n",
    "output_enc.append(out_enc1)\n",
    "output_enc.append(out_enc2)\n",
    "output_enc.append(out_enc3)\n",
    "output_enc.append(out_enc4)\n",
    "\n",
    "output_bridge = bridge(output_enc)\n",
    "print(f\"output_enc[0]:{output_bridge[0].shape}\")\n",
    "print(f\"output_enc[1]:{output_bridge[1].shape}\")\n",
    "print(f\"output_enc[2]:{output_bridge[2].shape}\")\n",
    "print(f\"output_enc[3]:{output_bridge[3].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e98c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
